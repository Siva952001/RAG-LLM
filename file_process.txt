import os
import pandas as pd
import numpy as np
import streamlit as st
from PyPDF2 import PdfReader
from docx import Document
from langchain.schema import Document as LangchainDocument
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

FILE_DIR = 'files'

def extract_text_from_pdf(file_path):
    text = ""
    with open(file_path, "rb") as file:
        reader = PdfReader(file)
        for page_num in range(len(reader.pages)):
            text += reader.pages[page_num].extract_text() + "\n"
    return text

def extract_text_from_docx(file_path):
    text = ""
    doc = Document(file_path)
    for para in doc.paragraphs:
        text += para.text + "\n"
    return text

def adjust_chunk_parameters(text):
    total_length = len(text)
    avg_sentence_length = sum(len(sentence) for sentence in text.split(".")) / max(len(text.split(".")), 1)

    if total_length < 5000:
        chunk_size = 600
        chunk_overlap = 100
    elif total_length < 20000:
        chunk_size = 800
        chunk_overlap = 200
    else:
        chunk_size = 1000
        chunk_overlap = 300

    if avg_sentence_length > 100:
        chunk_size += 500
        chunk_overlap += 100

    return chunk_size, chunk_overlap

def process_uploaded_files(files, directory):
    if files:
        for file in files:
            file_key = f"{file.name}_{file.size}"
            file_path = os.path.join(FILE_DIR, file.name)
            vectorstore_path = os.path.join(directory, f"{file.name}_vectorstore")

            # Check if file is already processed with the current embedding model
            if file_key in st.session_state.processed_files:
                st.sidebar.warning(f"⚠️ File '{file.name}' already exists. You can ask questions.")
            elif os.path.exists(vectorstore_path):
                try:
                    # Check embedding dimensionality before loading
                    st.session_state.vectorstore = Chroma(
                        persist_directory=vectorstore_path,
                        embedding_function=st.session_state.embedding_model
                    )
                    st.session_state.retriever = st.session_state.vectorstore.as_retriever()
                    st.session_state.processed_files[file_key] = True
                except ValueError as e:
                    st.sidebar.error(f"Error loading vector store for '{file.name}': {e}")
                    continue
            else:
                with open(file_path, "wb") as f:
                    f.write(file.getvalue())

                with st.spinner("Training started..."): 
                    # Extract content based on file type
                    if file.name.endswith('.xlsx') or file.name.endswith('.xls'):
                        xls = pd.ExcelFile(file_path)
                        all_content = "\n".join(["\n".join(map(str, pd.read_excel(xls, sheet).to_dict(orient='records'))) for sheet in xls.sheet_names])
                    elif file.name.endswith('.pdf'):
                        all_content = extract_text_from_pdf(file_path)
                    elif file.name.endswith('.docx'):
                        all_content = extract_text_from_docx(file_path)
                    else:
                        st.sidebar.warning(f"Unsupported file type: {file.name}")
                        continue

                    # Determine chunking parameters
                    chunk_size, chunk_overlap = adjust_chunk_parameters(all_content)
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap,
                        length_function=len
                    )

                    # Split text and create documents
                    splits = text_splitter.split_text(all_content)
                    documents = [LangchainDocument(page_content=split) for split in splits]

                    # Generate unique IDs for each chunk
                    ids = [f"{file.name}_chunk_{i}" for i in range(len(documents))]

                    # Create or update vector store
                    try:
                        st.session_state.vectorstore = Chroma.from_documents(
                            documents=documents,
                            embedding=st.session_state.embedding_model,
                            persist_directory=vectorstore_path,
                            ids=ids
                        )
                        st.session_state.vectorstore.persist()
                        st.session_state.processed_files[file_key] = True
                        st.session_state.retriever = st.session_state.vectorstore.as_retriever()
                        st.sidebar.success(f"File '{file.name}' processed successfully!")
                    except ValueError as e:
                        st.sidebar.error(f"Error processing '{file.name}': {e}")
                        continue
